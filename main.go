package main

import (
	"bufio"
	"context"
	"fmt"
	"os"
	"strings"

	"github.com/smallnest/langgraphgo/graph"
	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/llms/openai"
)

func main() {
	demo1()
	// demo4()
}

// original version
func demo1() {
	fmt.Println("=== Ollama DeepSeek 14B å¯¹è¯ Demo ===")
	fmt.Println("æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
	fmt.Println()

	// é…ç½® Ollamaï¼ˆOpenAI å…¼å®¹æ¨¡å¼ï¼‰
	// Ollama é»˜è®¤åœ¨ localhost:11434 è¿è¡Œï¼ŒOpenAI å…¼å®¹æ¥å£åœ¨ /v1
	model, err := openai.New(
		openai.WithBaseURL("http://localhost:11434/v1"),
		openai.WithModel("deepseek-r1:14b"),
		openai.WithToken("ollama"), // Ollama ä¸éœ€è¦çœŸå® token
	)
	if err != nil {
		fmt.Printf("âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: %v\n", err)
		fmt.Println("è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve")
		return
	}

	// åˆ›å»ºå¯¹è¯å›¾
	g := graph.NewStateGraph()

	// æ·»åŠ å¯¹è¯èŠ‚ç‚¹
	g.AddNode("chat", "ä¸ DeepSeek å¯¹è¯", func(ctx context.Context, state interface{}) (interface{}, error) {
		messages := state.([]llms.MessageContent)

		fmt.Print("ğŸ¤– DeepSeek æ€è€ƒä¸­...")

		// è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
		response, err := model.GenerateContent(ctx, messages,
			llms.WithTemperature(0.7),
			llms.WithMaxTokens(2000),
		)
		if err != nil {
			return nil, fmt.Errorf("ç”Ÿæˆå¤±è´¥: %w", err)
		}

		fmt.Print("\r")

		// æå–å›å¤å†…å®¹
		aiResponse := response.Choices[0].Content

		// è¿”å›æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨
		return append(messages, llms.TextParts(llms.ChatMessageTypeAI, aiResponse)), nil
	})

	// è®¾ç½®å›¾ç»“æ„
	g.AddEdge("chat", graph.END)
	g.SetEntryPoint("chat")

	// ç¼–è¯‘å›¾
	runnable, err := g.Compile()
	if err != nil {
		panic(err)
	}

	// å¯¹è¯å†å²
	var conversationHistory []llms.MessageContent

	// äº¤äº’å¼å¯¹è¯å¾ªç¯
	scanner := bufio.NewScanner(os.Stdin)
	ctx := context.Background()

	for {
		fmt.Print("ğŸ‘¤ ä½ : ")
		if !scanner.Scan() {
			break
		}

		userInput := strings.TrimSpace(scanner.Text())

		// æ£€æŸ¥é€€å‡ºå‘½ä»¤
		if userInput == "quit" || userInput == "exit" || userInput == "" {
			fmt.Println("ğŸ‘‹ å†è§ï¼")
			break
		}

		// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
		conversationHistory = append(conversationHistory,
			llms.TextParts(llms.ChatMessageTypeHuman, userInput))

		// æ‰§è¡Œå¯¹è¯
		result, err := runnable.Invoke(ctx, conversationHistory)
		if err != nil {
			fmt.Printf("âŒ é”™è¯¯: %v\n", err)
			continue
		}

		// æ›´æ–°å¯¹è¯å†å²
		conversationHistory = result.([]llms.MessageContent)

		// æ˜¾ç¤º AI å›å¤
		lastMessage := conversationHistory[len(conversationHistory)-1]
		if len(lastMessage.Parts) > 0 {
			if textPart, ok := lastMessage.Parts[0].(llms.TextContent); ok {
				fmt.Printf("ğŸ¤– DeepSeek: %s\n\n", textPart.Text)
			}
		}
	}
}

// improved version with ChatAgent
// func demo2() {
// 	fmt.Println("=== Ollama DeepSeek 14B ChatAgent Demo ===")
// 	fmt.Println("æç¤ºï¼šè¾“å…¥ 'quit' é€€å‡ºï¼Œ'history' æŸ¥çœ‹å†å²ï¼Œ'clear' æ¸…é™¤å†å²")
// 	fmt.Println()

// 	// é…ç½® Ollama
// 	llm, err := openai.New(
// 		openai.WithBaseURL("http://localhost:11434/v1"),
// 		openai.WithModel("deepseek-r1:14b"),
// 		openai.WithToken("ollama"),
// 	)
// 	if err != nil {
// 		fmt.Printf("âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: %v\n", err)
// 		return
// 	}

// 	// åˆ›å»º ChatAgentï¼ˆè‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²ï¼‰
// 	agent, err := prebuilt.NewChatAgent(llm, nil)
// 	if err != nil {
// 		fmt.Printf("âŒ åˆ›å»º Agent å¤±è´¥: %v\n", err)
// 		return
// 	}

// 	fmt.Printf("ğŸ“ ä¼šè¯ ID: %s\n\n", agent.ThreadID())

// 	// äº¤äº’å¾ªç¯
// 	scanner := bufio.NewScanner(os.Stdin)
// 	ctx := context.Background()

// 	for {
// 		fmt.Print("ğŸ‘¤ ä½ : ")
// 		if !scanner.Scan() {
// 			break
// 		}

// 		input := strings.TrimSpace(scanner.Text())

// 		switch input {
// 		case "quit", "exit", "":
// 			fmt.Println("ğŸ‘‹ å†è§ï¼")
// 			return

// 		case "history":
// 			// æ˜¾ç¤ºå¯¹è¯å†å²
// 			history := agent.GetHistory()
// 			fmt.Println("\nğŸ“œ å¯¹è¯å†å²:")
// 			for i, msg := range history {
// 				role := "æœªçŸ¥"
// 				if msg.Role == "human" {
// 					role = "ç”¨æˆ·"
// 				} else if msg.Role == "ai" {
// 					role = "AI"
// 				}
// 				fmt.Printf("  %d. [%s]: %v\n", i+1, role, msg.Parts)
// 			}
// 			fmt.Println()
// 			continue

// 		case "clear":
// 			agent.ClearHistory()
// 			fmt.Println("âœ… å†å²å·²æ¸…é™¤\n")
// 			continue
// 		}

// 		// å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
// 		fmt.Print("ğŸ¤– DeepSeek æ€è€ƒä¸­...")
// 		response, err := agent.Chat(ctx, input)
// 		if err != nil {
// 			fmt.Printf("\nâŒ é”™è¯¯: %v\n\n", err)
// 			continue
// 		}

// 		fmt.Printf("\rğŸ¤– DeepSeek: %s\n\n", response)
// 	}
// }

func demo3() {
	fmt.Println("=== Ollama DeepSeek æµå¼å¯¹è¯ Demo ===")
	fmt.Println("æç¤ºï¼šè¾“å…¥ 'quit' é€€å‡º")
	fmt.Println()

	// é…ç½®æ¨¡å‹
	model, err := openai.New(
		openai.WithBaseURL("http://localhost:11434/v1"),
		openai.WithModel("deepseek-r1:14b"),
		openai.WithToken("ollama"),
	)
	if err != nil {
		fmt.Printf("âŒ é”™è¯¯: %v\n", err)
		return
	}

	// åˆ›å»ºå›¾
	g := graph.NewStateGraph()

	g.AddNode("chat", "å¯¹è¯", func(ctx context.Context, state interface{}) (interface{}, error) {
		messages := state.([]llms.MessageContent)

		// ä½¿ç”¨æµå¼ API
		fmt.Print("ğŸ¤– DeepSeek: ")

		var fullResponse strings.Builder
		_, err := model.GenerateContent(ctx, messages,
			llms.WithTemperature(0.7),
			llms.WithMaxTokens(2000),
			llms.WithStreamingFunc(func(ctx context.Context, chunk []byte) error {
				text := string(chunk)
				fullResponse.WriteString(text)
				fmt.Print(text) // å®æ—¶æ‰“å°
				return nil
			}),
		)

		fmt.Println() // æ¢è¡Œ

		if err != nil {
			return nil, err
		}

		return append(messages, llms.TextParts(llms.ChatMessageTypeAI, fullResponse.String())), nil
	})

	g.AddEdge("chat", graph.END)
	g.SetEntryPoint("chat")

	runnable, _ := g.Compile()

	// å¯¹è¯å¾ªç¯
	scanner := bufio.NewScanner(os.Stdin)
	ctx := context.Background()
	var history []llms.MessageContent

	for {
		fmt.Print("ğŸ‘¤ ä½ : ")
		if !scanner.Scan() {
			break
		}

		input := strings.TrimSpace(scanner.Text())
		if input == "quit" || input == "exit" || input == "" {
			break
		}

		history = append(history, llms.TextParts(llms.ChatMessageTypeHuman, input))

		result, err := runnable.Invoke(ctx, history)
		if err != nil {
			fmt.Printf("âŒ é”™è¯¯: %v\n", err)
			continue
		}

		history = result.([]llms.MessageContent)
		fmt.Println()
	}

	fmt.Println("ğŸ‘‹ å†è§ï¼")
}

func demo4() {
	g := graph.NewMessageGraph()

	// æ·»åŠ ä¸€ä¸ªç®€å•çš„å¤„ç†èŠ‚ç‚¹
	g.AddNode("process", "chat", func(ctx context.Context, state interface{}) (interface{}, error) {
		// MessageGraph éœ€è¦ state æ˜¯ map[string]any ç±»å‹
		stateMap := state.(map[string]any)

		// è·å–è¾“å…¥æ¶ˆæ¯
		input, ok := stateMap["input"].(string)
		if !ok {
			return nil, fmt.Errorf("input not found or not a string")
		}

		// å¤„ç†æ¶ˆæ¯
		output := fmt.Sprintf("PROCESSED_%s", input)

		// è¿”å›æ›´æ–°åçš„çŠ¶æ€
		stateMap["output"] = output
		return stateMap, nil
	})

	// è®¾ç½®è¾¹ï¼šä» process èŠ‚ç‚¹åˆ° END ç»“æŸç‚¹
	g.AddEdge("process", graph.END)
	// è®¾ç½®å…¥å£ç‚¹
	g.SetEntryPoint("process")

	// ç¼–è¯‘å›¾ä»¥è·å¾—å¯æ‰§è¡Œå®ä¾‹
	runnable, err := g.Compile()
	if err != nil {
		panic(err)
	}

	// ä½¿ç”¨åˆå§‹çŠ¶æ€è°ƒç”¨å›¾ - å¿…é¡»æ˜¯ map[string]any
	initialState := map[string]any{
		"input": "hello_world",
	}

	result, err := runnable.Invoke(context.Background(), initialState)
	if err != nil {
		panic(err)
	}

	resultMap := result.(map[string]any)
	fmt.Printf("ç»“æœ: %v\n", resultMap["output"])
}
