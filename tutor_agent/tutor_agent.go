package tutor_agent

import (
	"bufio"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"math"
	"ollama-demo/hnsw"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/smallnest/langgraphgo/graph"
	"github.com/tmc/langchaingo/embeddings"
	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/llms/openai"
)

// For test: /Users/wangyang/gitclonefiles/datafusion/README.md
// For test: /Users/wangyang/gitclonefiles/ClickHouse/src/core/Settings.cpp
// For test: /Users/wangyang/hello_langchain/langgraphdemogo/test.md
// For test: /Users/wangyang/hello_langchain/langgraphdemogo/test2.md

// HNSWçš„Må‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ
// å¦‚ä½•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ï¼Ÿ
// ä½™å¼¦ç›¸ä¼¼åº¦çš„è®¡ç®—å…¬å¼
// æ¬§æ°è·ç¦»å’Œä½™å¼¦è·ç¦»æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
// RAGè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
// ä¸ºä»€ä¹ˆHNSWæ¯”å…¶ä»–ç®—æ³•å¿«ï¼Ÿ
// å‘é‡æ£€ç´¢å¯ä»¥ç”¨åœ¨å“ªäº›åœºæ™¯ï¼Ÿ
// å¦‚ä½•é€‰æ‹©åˆé€‚çš„Embeddingæ¨¡å‹ï¼Ÿ
// HNSWå’ŒIVFæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
// ä¸åŒåˆ†å—ç­–ç•¥çš„ä¼˜ç¼ºç‚¹

type LLMType int

const (
	OpenAI LLMType = iota
	Ollama

	DefaultDataDir = "./tutor_data"  // é»˜è®¤æ•°æ®ç›®å½•
	HNSWSubDir     = "hnsw_index"    // HNSWç´¢å¼•å­ç›®å½•
	MetadataFile   = "metadata.json" // å…ƒæ•°æ®æ–‡ä»¶
)

// æ·»åŠ æŒä¹…åŒ–å…ƒæ•°æ®ç»“æ„
type PersistentMetadata struct {
	DocumentChunks     []DocumentChunk   `json:"document_chunks"`
	NodeIDToChunkIndex map[int]int       `json:"node_id_to_chunk_index"`
	DocumentContents   map[string]string `json:"document_contents"`
	Dimension          int               `json:"dimension"`
	CreatedAt          string            `json:"created_at"`
	UpdatedAt          string            `json:"updated_at"`
}

type DocumentChunk struct {
	Content  string            // åˆ‡ç‰‡å†…å®¹
	Source   string            // æ¥æºæ–‡ä»¶
	ChunkID  int               // åˆ‡ç‰‡ID
	Metadata map[string]string // å…ƒæ•°æ®
}

// Tutor agent state definition
type TutorState struct {
	// Documents contents
	DocumentContents map[string]string

	// Document chunks and vector index
	DocumentChunks []DocumentChunk

	// HNSW Vector Index
	VectorIndex *hnsw.HNSWIndex

	// Embedding model
	Embedder embeddings.Embedder

	// æ–°å¢ï¼šNodeID åˆ° Chunk ç´¢å¼•çš„æ˜ å°„
	NodeIDToChunkIndex map[int]int

	// Generated by the Agent
	DocumentSummary string

	// Chat history
	Messages []llms.MessageContent

	// User input
	UserInput string

	// Whether the agent should continue
	ShouldContinue bool

	// Current teaching stage
	Stage string

	// æ–°å¢ï¼šæ•°æ®ç›®å½•è·¯å¾„
	DataDir string
}

type TutorAgent struct {
	model     llms.Model
	embedder  embeddings.Embedder
	graph     *graph.StateRunnable[TutorState]
	scanner   *bufio.Scanner
	dimension int    // Embedding dimension
	dataDir   string // æ–°å¢ï¼šæ•°æ®ç›®å½•
}

func NewTutorAgent(llmType LLMType) (*TutorAgent, error) {
	switch llmType {
	case OpenAI:
		return nil, errors.New("unsupported LLM type: OpenAI")
	case Ollama:
		return newOllamaTutorAgent()
	default:
		return nil, errors.New(fmt.Sprintf("unrecognized LLM type: %d", llmType))
	}
}

// ä¿®æ”¹ NewTutorAgentï¼Œæ·»åŠ å¯é€‰çš„æ•°æ®ç›®å½•å‚æ•°
func NewTutorAgentWithDataDir(llmType LLMType, dataDir string) (*TutorAgent, error) {
	if dataDir == "" {
		dataDir = DefaultDataDir
	}

	// ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(dataDir, 0755); err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: %v", err)
	}

	switch llmType {
	case OpenAI:
		return nil, errors.New("unsupported LLM type: OpenAI")
	case Ollama:
		return newOllamaTutorAgentWithDataDir(dataDir)
	default:
		return nil, errors.New(fmt.Sprintf("unrecognized LLM type: %d", llmType))
	}
}

// ä¿®æ”¹ newOllamaTutorAgent
func newOllamaTutorAgentWithDataDir(dataDir string) (*TutorAgent, error) {
	// Config Ollama
	model, err := openai.New(
		openai.WithBaseURL("http://localhost:11434/v1"),
		openai.WithModel("deepseek-r1:14b"),
		openai.WithToken("ollama"),
	)
	if err != nil {
		return nil, fmt.Errorf("create LLM failed: %v", err)
	}

	embeddingLLM, err := openai.New(
		openai.WithBaseURL("http://localhost:11434/v1"),
		openai.WithToken("ollama"),
		openai.WithModel("nomic-embed-text"),
		openai.WithEmbeddingModel("nomic-embed-text"),
	)
	if err != nil {
		return nil, fmt.Errorf("create embedding LLM failed: %v", err)
	}

	embedder, err := embeddings.NewEmbedder(embeddingLLM)
	if err != nil {
		return nil, fmt.Errorf("create embedder failed: %v", err)
	}

	agent := &TutorAgent{
		model:     model,
		embedder:  embedder,
		scanner:   bufio.NewScanner(os.Stdin),
		dimension: 768,
		dataDir:   dataDir, // ä¿å­˜æ•°æ®ç›®å½•
	}

	if err := agent.buildGraph(); err != nil {
		return nil, err
	}

	return agent, nil
}

// ä¿æŒå‘åå…¼å®¹
func newOllamaTutorAgent() (*TutorAgent, error) {
	return newOllamaTutorAgentWithDataDir(DefaultDataDir)
}

// æ–°å¢ï¼šä¿å­˜å…ƒæ•°æ®åˆ°æ–‡ä»¶
func (t *TutorAgent) saveMetadata(state TutorState) error {
	metadataPath := filepath.Join(state.DataDir, MetadataFile)

	metadata := PersistentMetadata{
		DocumentChunks:     state.DocumentChunks,
		NodeIDToChunkIndex: state.NodeIDToChunkIndex,
		DocumentContents:   state.DocumentContents,
		Dimension:          t.dimension,
		CreatedAt:          time.Now().Format(time.RFC3339),
		UpdatedAt:          time.Now().Format(time.RFC3339),
	}

	data, err := json.MarshalIndent(metadata, "", "  ")
	if err != nil {
		return fmt.Errorf("åºåˆ—åŒ–å…ƒæ•°æ®å¤±è´¥: %v", err)
	}

	if err := os.WriteFile(metadataPath, data, 0644); err != nil {
		return fmt.Errorf("å†™å…¥å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: %v", err)
	}

	return nil
}

// æ–°å¢ï¼šä»æ–‡ä»¶åŠ è½½å…ƒæ•°æ®
func (t *TutorAgent) loadMetadata(dataDir string) (*PersistentMetadata, error) {
	metadataPath := filepath.Join(dataDir, MetadataFile)

	data, err := os.ReadFile(metadataPath)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, nil // æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å› nil ä½†ä¸æŠ¥é”™
		}
		return nil, fmt.Errorf("è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: %v", err)
	}

	var metadata PersistentMetadata
	if err := json.Unmarshal(data, &metadata); err != nil {
		return nil, fmt.Errorf("è§£æå…ƒæ•°æ®å¤±è´¥: %v", err)
	}

	return &metadata, nil
}

// æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„æ•°æ®
func (t *TutorAgent) hasPersistedData(dataDir string) bool {
	metadataPath := filepath.Join(dataDir, MetadataFile)
	hnswDir := filepath.Join(dataDir, HNSWSubDir)

	// æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶å’ŒHNSWç›®å½•æ˜¯å¦éƒ½å­˜åœ¨
	_, err1 := os.Stat(metadataPath)
	_, err2 := os.Stat(filepath.Join(hnswDir, "nodes.lance"))

	return err1 == nil && err2 == nil
}

func (t *TutorAgent) buildGraph() error {
	g := graph.NewStateGraph[TutorState]()

	// 1. Load documents node
	g.AddNode("load_documents", "load documents", t.loadDocuments)

	g.AddNode("vectorize_documents", "vectorize documents", t.vectorizeDocuments)

	// 2. Analyze documents node
	g.AddNode("analyze_documents", "analyze documents", t.analyzeDocuments)

	// 3. Chat node
	g.AddNode("chat", "chat", t.chatWithRAG)

	// Set edges
	g.AddEdge("load_documents", "vectorize_documents")    // æ”¹ä¸ºå…ˆå‘é‡åŒ–
	g.AddEdge("vectorize_documents", "analyze_documents") // å‘é‡åŒ–åå†åˆ†æ
	g.AddEdge("analyze_documents", "chat")

	// no need the check continue node anymore
	// g.AddEdge("chat", "check_continue")

	// Conditional edge: decide the flow based on whether the user wants to continue
	g.AddConditionalEdge("chat", func(ctx context.Context, state TutorState) string {
		if state.ShouldContinue {
			return "chat" // continue chatting
		}
		return graph.END // end the session
	})

	// Set entry point
	g.SetEntryPoint("load_documents")

	// Compile the graph
	runnable, err := g.Compile()
	if err != nil {
		return err
	}

	t.graph = runnable
	return nil
}

// ä¿®æ”¹ loadDocuments å‡½æ•°ï¼Œæ·»åŠ æ•°æ®åŠ è½½é€»è¾‘
func (t *TutorAgent) loadDocuments(ctx context.Context, state TutorState) (TutorState, error) {
	// è®¾ç½®æ•°æ®ç›®å½•
	state.DataDir = t.dataDir

	fmt.Println("\nğŸ“š === æ™ºèƒ½åŠ©æ•™ç³»ç»Ÿï¼ˆRAG å¢å¼ºç‰ˆ + æŒä¹…åŒ–ï¼‰===")
	fmt.Println("æˆ‘å¯ä»¥å¸®åŠ©ä½ æ·±å…¥å­¦ä¹ å’Œç†è§£æ–‡æ¡£å†…å®¹ï¼")
	fmt.Println("ğŸ’¡ ä½¿ç”¨å‘é‡æœç´¢æŠ€æœ¯ï¼Œç²¾å‡†æ£€ç´¢ç›¸å…³å†…å®¹")
	fmt.Println("ğŸ’¾ æ”¯æŒæ•°æ®æŒä¹…åŒ–ï¼Œè‡ªåŠ¨ä¿å­˜å’Œæ¢å¤ç´¢å¼•")
	fmt.Println()

	// âœ¨ æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ•°æ®
	if t.hasPersistedData(state.DataDir) {
		fmt.Println("ğŸ” æ£€æµ‹åˆ°å·²ä¿å­˜çš„ç´¢å¼•æ•°æ®")
		fmt.Print("æ˜¯å¦åŠ è½½å·²æœ‰æ•°æ®ï¼Ÿ(y/nï¼Œé»˜è®¤ y): ")

		if t.scanner.Scan() {
			input := strings.TrimSpace(strings.ToLower(t.scanner.Text()))
			if input == "" || input == "y" || input == "yes" {
				// åŠ è½½å·²æœ‰æ•°æ®
				fmt.Println("\nğŸ“¥ æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„æ•°æ®...")
				if err := t.loadPersistedData(ctx, &state); err != nil {
					fmt.Printf("âš ï¸  åŠ è½½æ•°æ®å¤±è´¥: %v\n", err)
					fmt.Println("å°†é‡æ–°åˆ›å»ºç´¢å¼•...")
				} else {
					fmt.Printf("âœ… æˆåŠŸåŠ è½½ %d ä¸ªæ–‡æ¡£å—\n", len(state.DocumentChunks))
					fmt.Println("ğŸ’¡ æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' å¯ä»¥é€€å‡º")
					state.Stage = "documents_loaded"
					state.Embedder = t.embedder
					state.ShouldContinue = true
					// ç›´æ¥è·³åˆ°åˆ†æé˜¶æ®µ
					return state, nil
				}
			}
		}
	}

	// åŸæœ‰çš„æ–‡æ¡£åŠ è½½é€»è¾‘
	fmt.Print("è¯·è¾“å…¥è¦å­¦ä¹ çš„æ–‡æ¡£è·¯å¾„ï¼ˆå¤šä¸ªæ–‡ä»¶ç”¨é€—å·åˆ†éš”ï¼‰: ")
	if !t.scanner.Scan() {
		return state, fmt.Errorf("è¯»å–è¾“å…¥å¤±è´¥")
	}

	pathsInput := strings.TrimSpace(t.scanner.Text())
	if pathsInput == "" {
		return state, fmt.Errorf("æœªæä¾›æ–‡æ¡£è·¯å¾„")
	}

	// è§£æè·¯å¾„
	paths := strings.Split(pathsInput, ",")
	state.DocumentContents = make(map[string]string)

	// åŠ è½½æ¯ä¸ªæ–‡ä»¶
	for _, path := range paths {
		path = strings.TrimSpace(path)
		content, err := t.loadFile(path)
		if err != nil {
			fmt.Printf("âš ï¸  åŠ è½½æ–‡ä»¶ %s å¤±è´¥: %v\n", path, err)
			continue
		}
		state.DocumentContents[path] = content
		fmt.Printf("âœ… å·²åŠ è½½: %s (%d å­—ç¬¦)\n", filepath.Base(path), len(content))
	}

	if len(state.DocumentContents) == 0 {
		return state, fmt.Errorf("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
	}

	state.Stage = "documents_loaded"
	state.Embedder = t.embedder
	return state, nil
}

// æ–°å¢ï¼šåŠ è½½æŒä¹…åŒ–çš„æ•°æ®
func (t *TutorAgent) loadPersistedData(ctx context.Context, state *TutorState) error {
	// 1. åŠ è½½å…ƒæ•°æ®
	metadata, err := t.loadMetadata(state.DataDir)
	if err != nil {
		return err
	}
	if metadata == nil {
		return fmt.Errorf("å…ƒæ•°æ®ä¸å­˜åœ¨")
	}

	// 2. æ¢å¤æ–‡æ¡£å—å’Œæ˜ å°„
	state.DocumentChunks = metadata.DocumentChunks
	state.NodeIDToChunkIndex = metadata.NodeIDToChunkIndex
	state.DocumentContents = metadata.DocumentContents

	// 3. åŠ è½½ HNSW ç´¢å¼•
	hnswDir := filepath.Join(state.DataDir, HNSWSubDir)
	loadedIndex, err := hnsw.LoadHNSWFromLance(hnswDir)
	if err != nil {
		return fmt.Errorf("åŠ è½½ HNSW ç´¢å¼•å¤±è´¥: %v", err)
	}
	state.VectorIndex = loadedIndex

	// 4. è®¾ç½® embedder
	state.Embedder = t.embedder

	fmt.Printf("   ğŸ“Š æ–‡æ¡£å—: %d ä¸ª\n", len(state.DocumentChunks))
	fmt.Printf("   ğŸ—‚ï¸  æ–‡æ¡£: %d ä¸ª\n", len(state.DocumentContents))
	fmt.Printf("   ğŸ”— æ˜ å°„å…³ç³»: %d æ¡\n", len(state.NodeIDToChunkIndex))

	return nil
}

// loadFile åŠ è½½å•ä¸ªæ–‡ä»¶
func (t *TutorAgent) loadFile(path string) (string, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return "", err
	}
	return string(data), nil
}

// chunkText - æ–‡æ¡£åˆ‡å—
func (t *TutorAgent) chunkText(text string, chunkSize int, overlap int) []string {
	// æŒ‰æ®µè½åˆ†å‰²
	paragraphs := strings.Split(text, "\n\n")

	var chunks []string
	var currentChunk strings.Builder
	currentSize := 0

	for _, para := range paragraphs {
		para = strings.TrimSpace(para)
		if para == "" {
			continue
		}

		paraSize := len(para)

		// å¦‚æœå½“å‰æ®µè½åŠ ä¸Šå·²æœ‰å†…å®¹è¶…è¿‡ chunkSizeï¼Œä¿å­˜å½“å‰ chunk
		if currentSize > 0 && currentSize+paraSize > chunkSize {
			chunks = append(chunks, currentChunk.String())

			// ä¿ç•™ overlap éƒ¨åˆ†
			chunkText := currentChunk.String()
			if len(chunkText) > overlap {
				currentChunk.Reset()
				currentChunk.WriteString(chunkText[len(chunkText)-overlap:])
				currentChunk.WriteString("\n\n")
				currentSize = overlap
			} else {
				currentChunk.Reset()
				currentSize = 0
			}
		}

		currentChunk.WriteString(para)
		currentChunk.WriteString("\n\n")
		currentSize += paraSize
	}

	// æ·»åŠ æœ€åä¸€ä¸ª chunk
	if currentSize > 0 {
		chunks = append(chunks, currentChunk.String())
	}

	return chunks
}

// vectorizeDocuments - å‘é‡åŒ–æ–‡æ¡£
// ä¿®æ”¹ vectorizeDocuments å‡½æ•°ï¼Œæ·»åŠ ä¿å­˜é€»è¾‘
func (t *TutorAgent) vectorizeDocuments(ctx context.Context, state TutorState) (TutorState, error) {
	fmt.Println("\nğŸ”„ æ­£åœ¨å¤„ç†æ–‡æ¡£...")

	// åˆ›å»º HNSW ç´¢å¼•
	state.VectorIndex = hnsw.NewHNSW(hnsw.Config{
		M:              16,
		EfConstruction: 200,
		Dimension:      t.dimension,
		DistanceFunc:   hnsw.CosineDistance,
	})
	fmt.Printf("âœ… åˆ›å»ºå‘é‡ç´¢å¼• (dimension=%d)\n", t.dimension)

	// åˆ‡å—å‚æ•°
	chunkSize := 500
	overlap := 50

	state.DocumentChunks = []DocumentChunk{}
	nodeIDToChunkIndex := make(map[int]int)
	chunkID := 0

	// å¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
	for path, content := range state.DocumentContents {
		fmt.Printf("\nğŸ“„ å¤„ç†æ–‡æ¡£: %s\n", filepath.Base(path))

		chunks := t.chunkText(content, chunkSize, overlap)
		fmt.Printf("   åˆ‡åˆ†ä¸º %d ä¸ªå—\n", len(chunks))

		for i, chunkText := range chunks {
			var vector []float32
			var err error

			for retry := 0; retry < 3; retry++ {
				vector, err = state.Embedder.EmbedQuery(ctx, chunkText)
				if err == nil {
					break
				}
				if retry < 2 {
					time.Sleep(time.Duration(100*(retry+1)) * time.Millisecond)
				}
			}
			if err != nil {
				fmt.Printf("âš ï¸  å— %d å‘é‡åŒ–å¤±è´¥: %v\n", i, err)
				continue
			}

			vector32 := make([]float32, len(vector))
			for j, v := range vector {
				vector32[j] = float32(v)
			}

			nodeID, err := state.VectorIndex.Add(vector32)
			if err != nil {
				fmt.Printf("âš ï¸  å— %d æ·»åŠ åˆ°ç´¢å¼•å¤±è´¥: %v\n", i, err)
				continue
			}

			chunkIndex := len(state.DocumentChunks)
			nodeIDToChunkIndex[nodeID] = chunkIndex

			chunk := DocumentChunk{
				Content: chunkText,
				Source:  filepath.Base(path),
				ChunkID: chunkID,
				Metadata: map[string]string{
					"source": filepath.Base(path),
					"nodeID": fmt.Sprintf("%d", nodeID),
				},
			}
			state.DocumentChunks = append(state.DocumentChunks, chunk)
			chunkID++

			if (i+1)%10 == 0 || i == len(chunks)-1 {
				fmt.Printf("   è¿›åº¦: %d/%d å—å·²å‘é‡åŒ–\n", i+1, len(chunks))
			}
		}
	}

	state.NodeIDToChunkIndex = nodeIDToChunkIndex

	fmt.Printf("\nâœ… å‘é‡åŒ–å®Œæˆï¼æ€»å…±å¤„ç† %d ä¸ªæ–‡æ¡£å—\n", len(state.DocumentChunks))

	// âœ¨âœ¨âœ¨ æ–°å¢ï¼šä¿å­˜æ•°æ®åˆ°ç£ç›˜ âœ¨âœ¨âœ¨
	fmt.Print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•æ•°æ®...")
	if err := t.savePersistedData(state); err != nil {
		fmt.Printf("\nâš ï¸  ä¿å­˜å¤±è´¥: %v\n", err)
		fmt.Println("   ï¼ˆä¸å½±å“å½“å‰ä¼šè¯ï¼Œä½†ä¸‹æ¬¡å¯åŠ¨éœ€è¦é‡æ–°åˆ›å»ºç´¢å¼•ï¼‰")
	} else {
		fmt.Println(" å®Œæˆï¼")
		fmt.Printf("   æ•°æ®å·²ä¿å­˜åˆ°: %s\n", state.DataDir)
	}

	state.Stage = "vectorization_complete"
	return state, nil
}

// ä¿®æ”¹ savePersistedData å‡½æ•°ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
func (t *TutorAgent) savePersistedData(state TutorState) error {
	// âœ¨ æ–°å¢ï¼šç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(state.DataDir, 0755); err != nil {
		return fmt.Errorf("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: %v", err)
	}

	// 1. ä¿å­˜å…ƒæ•°æ®
	if err := t.saveMetadata(state); err != nil {
		return fmt.Errorf("ä¿å­˜å…ƒæ•°æ®å¤±è´¥: %v", err)
	}

	// 2. ä¿å­˜ HNSW ç´¢å¼•
	hnswDir := filepath.Join(state.DataDir, HNSWSubDir)
	if err := os.MkdirAll(hnswDir, 0755); err != nil {
		return fmt.Errorf("åˆ›å»º HNSW ç›®å½•å¤±è´¥: %v", err)
	}

	if err := state.VectorIndex.SaveToLance(hnswDir); err != nil {
		return fmt.Errorf("ä¿å­˜ HNSW ç´¢å¼•å¤±è´¥: %v", err)
	}

	return nil
}

// analyzeDocuments ä½¿ç”¨ AI åˆ†ææ–‡æ¡£å†…å®¹
func (t *TutorAgent) analyzeDocuments(ctx context.Context, state TutorState) (TutorState, error) {
	fmt.Println("\nğŸ” æ­£åœ¨åˆ†ææ–‡æ¡£å†…å®¹...")
	totalChars := 0
	for _, content := range state.DocumentContents {
		totalChars += len(content)
	}

	// æ„å»ºæ–‡æ¡£å†…å®¹æ‘˜è¦
	var docsBuilder strings.Builder
	docsBuilder.WriteString("ä»¥ä¸‹æ˜¯éœ€è¦å­¦ä¹ çš„æ–‡æ¡£å†…å®¹ï¼š\n\n")

	processedChars := 0
	fileCount := 0
	totalFiles := len(state.DocumentContents)

	for path, content := range state.DocumentContents {
		fileCount++
		fmt.Printf("ğŸ“„ å¤„ç†æ–‡ä»¶ [%d/%d]: %s\n", fileCount, totalFiles, filepath.Base(path))

		docsBuilder.WriteString(fmt.Sprintf("=== æ–‡ä»¶: %s ===\n", filepath.Base(path)))

		// å¦‚æœæ–‡æ¡£å¤ªé•¿ï¼Œæˆªå–å‰é¢éƒ¨åˆ†
		contentToAdd := content
		if len(content) > 2000 {
			contentToAdd = content[:2000]
			docsBuilder.WriteString(contentToAdd)
			docsBuilder.WriteString("\n\n[æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰ 2000 å­—ç¬¦]\n\n")
		} else {
			docsBuilder.WriteString(contentToAdd)
			docsBuilder.WriteString("\n\n")
		}

		processedChars += len(content)
		progress := float64(processedChars) / float64(totalChars) * 100
		fmt.Printf("   â³ åŠ è½½è¿›åº¦: %.1f%% (%d/%d å­—ç¬¦)\n", progress, processedChars, totalChars)
	}

	fmt.Println("\nâœ… æ–‡ä»¶åŠ è½½å®Œæˆï¼Œæ­£åœ¨æäº¤ç»™åŠ©æ•™åˆ†æ...")

	// è®© AI åˆ†ææ–‡æ¡£
	analysisPrompt := docsBuilder.String() + fmt.Sprintf(`

æ–‡æ¡£å·²è¢«åˆ‡åˆ†ä¸º %d ä¸ªå—å¹¶å‘é‡åŒ–ï¼Œæ”¯æŒæ™ºèƒ½æ£€ç´¢ã€‚

è¯·ä½œä¸ºä¸€ä½ä¸“ä¸šçš„åŠ©æ•™ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. ç®€è¦æ¦‚è¿°è¿™äº›æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œæ ¸å¿ƒæ¦‚å¿µ
2. åˆ—å‡ºæ–‡æ¡£ä¸­çš„é‡ç‚¹çŸ¥è¯†ç‚¹
3. è¯´æ˜ä½ å°†å¦‚ä½•å¸®åŠ©å­¦ä¹ è€…ç†è§£è¿™äº›å†…å®¹

è¯·ç”¨å‹å¥½ã€æ˜“æ‡‚çš„è¯­è¨€å›å¤ã€‚`, len(state.DocumentChunks))

	messages := []llms.MessageContent{
		llms.TextParts(llms.ChatMessageTypeHuman, analysisPrompt),
	}

	fmt.Println() // æ¢è¡Œï¼Œè®©è¾“å‡ºæ›´æ¸…æ™°
	fmt.Println("ğŸ¤– åŠ©æ•™æ­£åœ¨æ·±åº¦åˆ†ææ–‡æ¡£...")
	fmt.Println(strings.Repeat("-", 60))

	var summaryBuilder strings.Builder
	var isThinking bool
	var charCount int

	response, err := t.model.GenerateContent(ctx, messages,
		llms.WithTemperature(0.7),
		llms.WithMaxTokens(2000),
		llms.WithStreamingFunc(func(ctx context.Context, chunk []byte) error {
			text := string(chunk)

			// æ£€æµ‹æ€è€ƒè¿‡ç¨‹æ ‡è®°
			if strings.Contains(text, "<think>") {
				isThinking = true
				fmt.Print("ğŸ’­ åŠ©æ•™æ€è€ƒ: ")
				return nil
			}
			if strings.Contains(text, "</think>") {
				isThinking = false
				fmt.Println("")
				return nil
			}

			// è¾“å‡ºæ€è€ƒè¿‡ç¨‹æˆ–æœ€ç»ˆå†…å®¹
			if isThinking {
				fmt.Print(text)
			} else {
				// è¿‡æ»¤æ‰æ ‡ç­¾
				cleanText := strings.ReplaceAll(text, "<think>", "")
				cleanText = strings.ReplaceAll(cleanText, "</think>", "")
				if cleanText != "" {
					summaryBuilder.WriteString(cleanText)
					charCount += len(cleanText)
					fmt.Print(cleanText)
				}
			}

			return nil
		}),
	)
	if err != nil {
		return state, fmt.Errorf("åˆ†ææ–‡æ¡£å¤±è´¥: %v", err)
	}

	// å¦‚æœæœ‰æµå¼è¾“å‡ºï¼Œä½¿ç”¨ summaryBuilder çš„å†…å®¹ï¼Œå¦åˆ™ä½¿ç”¨ response
	if summaryBuilder.Len() > 0 {
		state.DocumentSummary = summaryBuilder.String()
	} else {
		state.DocumentSummary = response.Choices[0].Content
	}

	state.Messages = []llms.MessageContent{
		llms.TextParts(llms.ChatMessageTypeSystem,
			fmt.Sprintf(`ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŠ©æ•™ï¼Œå¸®åŠ©å­¦ä¹ è€…æ·±å…¥ç†è§£æ–‡æ¡£å†…å®¹ã€‚

æ–‡æ¡£å·²è¢«åˆ‡åˆ†ä¸º %d ä¸ªå—å¹¶å‘é‡åŒ–å­˜å‚¨ã€‚
å½“å›ç­”é—®é¢˜æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µæä¾›ç»™ä½ ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹å›ç­”å­¦ä¹ è€…çš„é—®é¢˜
2. æä¾›æ·±å…¥çš„è§£é‡Šå’Œç¤ºä¾‹
3. å¼•å¯¼å­¦ä¹ è€…æ€è€ƒå’Œæ¢ç´¢
4. ç”¨æ¸…æ™°ã€å‹å¥½çš„è¯­è¨€äº¤æµ

è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚`, len(state.DocumentChunks))),
		llms.TextParts(llms.ChatMessageTypeAI, state.DocumentSummary),
	}

	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Printf("âœ… åˆ†æå®Œæˆï¼æ–‡æ¡£å·²å‡†å¤‡å°±ç»ªï¼ˆ%d ä¸ªå‘é‡å—ï¼‰\n", len(state.DocumentChunks))
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("\nğŸ’¡ æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' å¯ä»¥é€€å‡º")

	state.Stage = "analysis_complete"
	state.ShouldContinue = true
	return state, nil
}

// æ–°å¢å‡½æ•°ï¼šretrieveRelevantChunks - æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
func (t *TutorAgent) retrieveRelevantChunks(ctx context.Context, state TutorState, query string, topK int) ([]DocumentChunk, error) {
	// å°†æŸ¥è¯¢å‘é‡åŒ–
	queryVector, err := state.Embedder.EmbedQuery(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥: %v", err)
	}

	// è½¬æ¢ä¸º float32
	queryVector32 := make([]float32, len(queryVector))
	for i, v := range queryVector {
		queryVector32[i] = float32(v)
	}

	// åœ¨ HNSW ç´¢å¼•ä¸­æœç´¢
	results, err := state.VectorIndex.Search(queryVector32, topK, 100)
	if err != nil {
		return nil, fmt.Errorf("å‘é‡æœç´¢å¤±è´¥: %v", err)
	}

	// è·å–å¯¹åº”çš„æ–‡æ¡£å—
	var relevantChunks []DocumentChunk
	for _, result := range results {
		// âœ¨ ä½¿ç”¨æ˜ å°„è¡¨è·å–æ­£ç¡®çš„ç´¢å¼•
		chunkIndex, exists := state.NodeIDToChunkIndex[result.ID]
		if !exists {
			fmt.Printf("âš ï¸  è­¦å‘Šï¼šNodeID %d åœ¨æ˜ å°„è¡¨ä¸­ä¸å­˜åœ¨\n", result.ID)
			continue
		}

		if chunkIndex >= 0 && chunkIndex < len(state.DocumentChunks) {
			chunk := state.DocumentChunks[chunkIndex] // â­ ä½¿ç”¨æ˜ å°„åçš„ç´¢å¼•
			similarity := 1.0 - result.Distance/2.0
			chunk.Metadata["similarity"] = fmt.Sprintf("%.4f", similarity)
			relevantChunks = append(relevantChunks, chunk)
		} else {
			fmt.Printf("âš ï¸  è­¦å‘Šï¼šæ˜ å°„åçš„ç´¢å¼• %d è¶…å‡ºèŒƒå›´ [0, %d)\n", chunkIndex, len(state.DocumentChunks))
		}
	}

	return relevantChunks, nil
}

// æ–°å¢å‡½æ•°ï¼šchatWithRAG - æ”¯æŒ RAG çš„å¯¹è¯äº¤äº’
// ï¼ˆæ›¿ä»£åŸæ¥çš„ chat å‡½æ•°ï¼‰
func (t *TutorAgent) chatWithRAG(ctx context.Context, state TutorState) (TutorState, error) {
	// è·å–ç”¨æˆ·è¾“å…¥
	fmt.Print("\nğŸ’¬ ä½ çš„é—®é¢˜: ")
	if !t.scanner.Scan() {
		state.ShouldContinue = false
		return state, nil
	}

	userInput := strings.TrimSpace(t.scanner.Text())
	state.UserInput = userInput

	// æ£€æŸ¥é€€å‡ºå‘½ä»¤
	if userInput == "quit" || userInput == "exit" {
		state.ShouldContinue = false
		fmt.Println("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ™ºèƒ½åŠ©æ•™ç³»ç»Ÿï¼ç¥å­¦ä¹ æ„‰å¿«ï¼")
		return state, nil
	}

	// å¦‚æœç”¨æˆ·ç›´æ¥å›è½¦ï¼ˆç©ºè¾“å…¥ï¼‰ï¼Œæç¤ºé‡æ–°è¾“å…¥
	if userInput == "" {
		fmt.Println("âš ï¸  è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ–è¾“å…¥ 'quit'/'exit' é€€å‡º")
		state.ShouldContinue = true
		return state, nil
	}

	// âœ¨âœ¨âœ¨ æ–°å¢ï¼šRAG æ£€ç´¢ç›¸å…³æ–‡æ¡£å— âœ¨âœ¨âœ¨
	fmt.Print("ğŸ” æ£€ç´¢ç›¸å…³å†…å®¹...")
	relevantChunks, err := t.retrieveRelevantChunks(ctx, state, userInput, 3)
	if err != nil {
		fmt.Printf("\nâš ï¸  æ£€ç´¢å¤±è´¥: %vï¼Œå°†ä½¿ç”¨ä¸€èˆ¬çŸ¥è¯†å›ç­”\n", err)
		relevantChunks = []DocumentChunk{}
	} else {
		fmt.Printf(" æ‰¾åˆ° %d ä¸ªç›¸å…³ç‰‡æ®µ\n", len(relevantChunks))
	}

	// âœ¨âœ¨âœ¨ æ–°å¢ï¼šæ„å»ºå¢å¼ºçš„æç¤ºè¯ âœ¨âœ¨âœ¨
	var contextBuilder strings.Builder
	contextBuilder.WriteString("åŸºäºä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n")

	for i, chunk := range relevantChunks {
		similarity := chunk.Metadata["similarity"]
		contextBuilder.WriteString(fmt.Sprintf("--- ç›¸å…³ç‰‡æ®µ %d (ç›¸ä¼¼åº¦: %s, æ¥æº: %s) ---\n",
			i+1, similarity, chunk.Source))
		contextBuilder.WriteString(chunk.Content)
		contextBuilder.WriteString("\n\n")
	}

	contextBuilder.WriteString(fmt.Sprintf("é—®é¢˜: %s\n\n", userInput))
	contextBuilder.WriteString("è¯·åŸºäºä¸Šè¿°ç›¸å…³å†…å®¹ï¼Œç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœä¸Šè¿°å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶å°½å¯èƒ½æä¾›å¸®åŠ©ã€‚")

	// âœ¨ ä¿®æ”¹ï¼šæ·»åŠ ç”¨æˆ·æ¶ˆæ¯æ—¶ä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡
	state.Messages = append(state.Messages,
		llms.TextParts(llms.ChatMessageTypeHuman, contextBuilder.String()))

	fmt.Println() // æ¢è¡Œ

	var responseBuilder strings.Builder
	var isThinking bool
	var hasStartedOutput bool

	response, err := t.model.GenerateContent(ctx, state.Messages,
		llms.WithTemperature(0.7),
		llms.WithMaxTokens(3000),
		llms.WithStreamingFunc(func(ctx context.Context, chunk []byte) error {
			text := string(chunk)

			// æ£€æµ‹æ€è€ƒè¿‡ç¨‹çš„å¼€å§‹æ ‡è®°
			if strings.Contains(text, "<think>") {
				isThinking = true
				fmt.Print("ğŸ’­ åŠ©æ•™æ€è€ƒ: ")
				hasStartedOutput = true
				return nil
			}

			// æ£€æµ‹æ€è€ƒè¿‡ç¨‹çš„ç»“æŸæ ‡è®°
			if strings.Contains(text, "</think>") {
				isThinking = false
				fmt.Println("")
				fmt.Print("ğŸ“ åŠ©æ•™: ")
				return nil
			}

			// è¾“å‡ºæ€è€ƒè¿‡ç¨‹æˆ–æœ€ç»ˆç­”æ¡ˆ
			if isThinking {
				// æ‰“å°æ€è€ƒè¿‡ç¨‹ï¼ˆè¿‡æ»¤æ ‡ç­¾ï¼‰
				cleanText := strings.ReplaceAll(text, "<think>", "")
				fmt.Print(cleanText)
			} else {
				// è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
				if !hasStartedOutput {
					fmt.Print("ğŸ“ åŠ©æ•™: ")
					hasStartedOutput = true
				}
				// è¿‡æ»¤æ ‡ç­¾
				cleanText := strings.ReplaceAll(text, "</think>", "")
				if cleanText != "" {
					fmt.Print(cleanText)
					responseBuilder.WriteString(cleanText)
				}
			}

			return nil
		}),
	)
	if err != nil {
		fmt.Printf("\nâŒ ç”Ÿæˆå›å¤å¤±è´¥: %v\n", err)
		// å³ä½¿å‡ºé”™ä¹Ÿç»§ç»­å¯¹è¯
		state.ShouldContinue = true
		return state, nil
	}

	fmt.Println() // å›å¤ç»“æŸåæ¢è¡Œ
	if len(relevantChunks) > 0 {
		fmt.Println("\nğŸ“š å¼•ç”¨æ¥æº:")
		for i, chunk := range relevantChunks {
			similarity := chunk.Metadata["similarity"]
			fmt.Printf("  [%d] %s (ç›¸ä¼¼åº¦: %s)\n", i+1, chunk.Source, similarity)
		}
	}

	// è·å– AI å›å¤å†…å®¹
	var aiResponse string
	if responseBuilder.Len() > 0 {
		aiResponse = responseBuilder.String()
	} else {
		aiResponse = response.Choices[0].Content
	}

	// æ·»åŠ  AI å›å¤åˆ°å†å²
	state.Messages = append(state.Messages,
		llms.TextParts(llms.ChatMessageTypeAI, aiResponse))

	if len(state.Messages) > 21 { // 1 system + 1 initial AI + 20 messages (10 è½®å¯¹è¯)
		state.Messages = append(state.Messages[:2], state.Messages[len(state.Messages)-20:]...)
	}

	// ç»§ç»­å¯¹è¯å¾ªç¯
	state.ShouldContinue = true
	state.Stage = "chat_complete"
	return state, nil
}

// chat å¯¹è¯äº¤äº’ - æ”¯æŒå¤šè½®å¯¹è¯
func (t *TutorAgent) chat(ctx context.Context, state TutorState) (TutorState, error) {
	// è·å–ç”¨æˆ·è¾“å…¥
	fmt.Print("\nğŸ’¬ ä½ çš„é—®é¢˜: ")
	if !t.scanner.Scan() {
		state.ShouldContinue = false
		return state, nil
	}

	userInput := strings.TrimSpace(t.scanner.Text())
	state.UserInput = userInput

	// æ£€æŸ¥é€€å‡ºå‘½ä»¤
	if userInput == "quit" || userInput == "exit" {
		state.ShouldContinue = false
		fmt.Println("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ™ºèƒ½åŠ©æ•™ç³»ç»Ÿï¼ç¥å­¦ä¹ æ„‰å¿«ï¼")
		return state, nil
	}

	// å¦‚æœç”¨æˆ·ç›´æ¥å›è½¦ï¼ˆç©ºè¾“å…¥ï¼‰ï¼Œæç¤ºé‡æ–°è¾“å…¥
	if userInput == "" {
		fmt.Println("âš ï¸  è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ–è¾“å…¥ 'quit'/'exit' é€€å‡º")
		state.ShouldContinue = true
		return state, nil
	}

	// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
	state.Messages = append(state.Messages,
		llms.TextParts(llms.ChatMessageTypeHuman, userInput))

	// ä¿®æ”¹ä¸ºï¼š
	fmt.Println() // æ¢è¡Œ

	var responseBuilder strings.Builder
	var isThinking bool
	var hasStartedOutput bool

	response, err := t.model.GenerateContent(ctx, state.Messages,
		llms.WithTemperature(0.7),
		llms.WithMaxTokens(3000),
		llms.WithStreamingFunc(func(ctx context.Context, chunk []byte) error {
			text := string(chunk)

			// æ£€æµ‹æ€è€ƒè¿‡ç¨‹çš„å¼€å§‹æ ‡è®°
			if strings.Contains(text, "<think>") {
				isThinking = true
				fmt.Print("ğŸ’­ åŠ©æ•™æ€è€ƒ: ")
				hasStartedOutput = true
				return nil
			}

			// æ£€æµ‹æ€è€ƒè¿‡ç¨‹çš„ç»“æŸæ ‡è®°
			if strings.Contains(text, "</think>") {
				isThinking = false
				fmt.Println("")
				fmt.Print("ğŸ“ åŠ©æ•™: ")
				return nil
			}

			// è¾“å‡ºæ€è€ƒè¿‡ç¨‹æˆ–æœ€ç»ˆç­”æ¡ˆ
			if isThinking {
				// æ‰“å°æ€è€ƒè¿‡ç¨‹ï¼ˆè¿‡æ»¤æ ‡ç­¾ï¼‰
				cleanText := strings.ReplaceAll(text, "<think>", "")
				fmt.Print(cleanText)
			} else {
				// è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
				if !hasStartedOutput {
					fmt.Print("ğŸ“ åŠ©æ•™: ")
					hasStartedOutput = true
				}
				// è¿‡æ»¤æ ‡ç­¾
				cleanText := strings.ReplaceAll(text, "</think>", "")
				if cleanText != "" {
					fmt.Print(cleanText)
					responseBuilder.WriteString(cleanText)
				}
			}

			return nil
		}),
	)
	if err != nil {
		fmt.Printf("\nâŒ ç”Ÿæˆå›å¤å¤±è´¥: %v\n", err)
		// å³ä½¿å‡ºé”™ä¹Ÿç»§ç»­å¯¹è¯
		state.ShouldContinue = true
		return state, nil
	}

	fmt.Println() // å›å¤ç»“æŸåæ¢è¡Œç¤º

	// è·å– AI å›å¤å†…å®¹
	var aiResponse string
	if responseBuilder.Len() > 0 {
		aiResponse = responseBuilder.String()
	} else {
		aiResponse = response.Choices[0].Content
	}

	// æ·»åŠ  AI å›å¤åˆ°å†å²
	state.Messages = append(state.Messages,
		llms.TextParts(llms.ChatMessageTypeAI, aiResponse))

	// ç»§ç»­å¯¹è¯å¾ªç¯
	state.ShouldContinue = true
	state.Stage = "chat_complete"
	return state, nil
}

// Run è¿è¡ŒåŠ©æ•™ç³»ç»Ÿ
func (t *TutorAgent) Run() error {
	ctx := context.Background()

	// åˆå§‹åŒ–çŠ¶æ€
	initialState := TutorState{
		DocumentContents:   make(map[string]string),
		DocumentChunks:     []DocumentChunk{},
		NodeIDToChunkIndex: make(map[int]int),
		Messages:           []llms.MessageContent{},
		ShouldContinue:     true,
		Stage:              "init",
		VectorIndex:        nil,
		Embedder:           nil,
	}

	_, err := t.graph.Invoke(ctx, initialState)
	if err != nil {
		return fmt.Errorf("æ‰§è¡Œå¤±è´¥: %v", err)
	}

	return nil
}

// æ–°å¢å‡½æ•°ï¼šcosineSimilarity - ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
// ï¼ˆç”¨äºéªŒè¯ï¼Œå¯é€‰ï¼‰
func cosineSimilarity(a, b []float32) float64 {
	if len(a) != len(b) {
		return 0
	}

	var dotProduct, normA, normB float64
	for i := range a {
		dotProduct += float64(a[i]) * float64(b[i])
		normA += float64(a[i]) * float64(a[i])
		normB += float64(b[i]) * float64(b[i])
	}

	if normA == 0 || normB == 0 {
		return 0
	}

	return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}
