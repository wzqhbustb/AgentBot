{
  "document_chunks": [
    {
      "Content": "# 向量数据库技术白皮书\n\n## 第一章：向量数据库基础\n\n### 1.1 什么是向量数据库\n\n向量数据库是一种专门用于存储和检索高维向量数据的数据库系统。与传统关系型数据库不同，向量数据库通过计算向量之间的相似度来查找相关数据，而不是精确匹配。\n\n",
      "Source": "test2.md",
      "ChunkID": 0,
      "Metadata": {
        "nodeID": "0",
        "source": "test2.md"
      }
    },
    {
      "Content": "来查找相关数据，而不是精确匹配。\n\n\n\n向量数据库的核心优势包括：\n- 语义搜索能力：能够理解查询的真实意图\n- 高维数据处理：可处理数百甚至上千维度的向量\n- 近似最近邻搜索：通过ANN算法实现毫秒级查询\n- 多模态支持：可以处理文本、图像、音频等多种数据类型\n\n### 1.2 向量表示\n\n",
      "Source": "test2.md",
      "ChunkID": 1,
      "Metadata": {
        "nodeID": "1",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd\ufffd频等多种数据类型\n\n### 1.2 向量表示\n\n\n\n在机器学习中，任何数据都可以被转换为向量表示。文本通过Embedding模型转换为稠密向量，这个过程称为向量化。常见的文本向量维度有：\n- Word2Vec: 100-300维\n- BERT系列: 768维或1024维\n- OpenAI ada-002: 1536维\n- nomic-embed-text: 768维\n\n向量化后，语义相近的内容在向量空间中距离更近。\n\n## 第二章：相似度度量方法\n\n### 2.1 欧氏距离 (L2 Distance)\n\n",
      "Source": "test2.md",
      "ChunkID": 2,
      "Metadata": {
        "nodeID": "2",
        "source": "test2.md"
      }
    },
    {
      "Content": "度量方法\n\n### 2.1 欧氏距离 (L2 Distance)\n\n\n\n欧氏距离是最直观的距离度量方法，计算两点间的直线距离。公式为：\n\n$$d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n\n欧氏距离适用于关注绝对差异的场景，但对向量长度敏感。\n\n### 2.2 余弦相似度 (Cosine Similarity)\n\n余弦相似度衡量两个向量方向的相似程度，取值范围为[-1, 1]。公式为：\n\n$$\\text{cosine}(x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$$\n\n",
      "Source": "test2.md",
      "ChunkID": 3,
      "Metadata": {
        "nodeID": "3",
        "source": "test2.md"
      }
    },
    {
      "Content": "e}(x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$$\n\n\n\n余弦相似度是文本检索最常用的度量方法，因为它：\n- 不受向量长度影响\n- 关注方向而非大小\n- 计算效率高\n\n余弦距离定义为：$d = 1 - \\text{cosine}(x, y)$\n\n### 2.3 点积 (Dot Product)\n\n点积相似度计算简单，适用于归一化向量。对于单位向量，点积等价于余弦相似度。\n\n$$\\text{dot}(x, y) = \\sum_{i=1}^{n} x_i \\cdot y_i$$\n\n## 第三章：向量索引算法\n\n### 3.1 HNSW算法原理\n\n",
      "Source": "test2.md",
      "ChunkID": 4,
      "Metadata": {
        "nodeID": "4",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd：向量索引算法\n\n### 3.1 HNSW算法原理\n\n\n\nHNSW (Hierarchical Navigable Small World) 是目前最先进的ANN算法之一，由Yury Malkov在2016年提出。\n\n#### 3.1.1 算法核心思想\n\nHNSW构建了一个多层图结构：\n- 底层(Layer 0)包含所有节点，形成稠密连接\n- 高层节点稀疏，作为\"高速公路\"加速搜索\n- 层数遵循指数分布：P(level) = (1/M)^level\n\n#### 3.1.2 关键参数\n\n",
      "Source": "test2.md",
      "ChunkID": 5,
      "Metadata": {
        "nodeID": "5",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffdP(level) = (1/M)^level\n\n#### 3.1.2 关键参数\n\n\n\n**M参数**：每个节点的最大连接数\n- M越大，召回率越高，但内存和构建时间增加\n- 推荐值：4-64，常用16\n- 底层(Layer 0)最大连接数为2*M\n\n**efConstruction参数**：构建时的搜索宽度\n- 控制构建质量，值越大质量越好\n- 推荐值：100-500\n- 影响构建时间，但不影响查询速度\n\n",
      "Source": "test2.md",
      "ChunkID": 6,
      "Metadata": {
        "nodeID": "6",
        "source": "test2.md"
      }
    },
    {
      "Content": "\n- 影响构建时间，但不影响查询速度\n\n\n\n**efSearch参数**：查询时的搜索宽度\n- 控制召回率与速度的平衡\n- 动态调整，可在查询时指定\n- 通常设为 max(k, efConstruction)\n\n#### 3.1.3 性能特点\n\nHNSW的时间复杂度：\n- 构建时间：O(N * log(N) * M * efConstruction)\n- 查询时间：O(log(N) * efSearch)\n- 空间复杂度：O(N * M)\n\n在100万向量、768维数据集上的典型性能：\n- QPS: 1000-5000\n- 召回率@10: 95%+\n- 平均查询延迟: 1-5ms\n\n",
      "Source": "test2.md",
      "ChunkID": 7,
      "Metadata": {
        "nodeID": "7",
        "source": "test2.md"
      }
    },
    {
      "Content": "- 召回率@10: 95%+\n- 平均查询延迟: 1-5ms\n\n\n\n### 3.2 其他索引算法对比\n\n**IVF (Inverted File Index)**\n- 基于聚类的分区方法\n- 查询时只搜索部分分区\n- 适合超大规模数据集\n- 召回率略低于HNSW\n\n**LSH (Locality Sensitive Hashing)**\n- 基于哈希的近似方法\n- 构建速度快，内存占用小\n- 召回率较低，适合对精度要求不高的场景\n\n**Product Quantization (PQ)**\n- 向量压缩技术\n- 大幅降低内存占用\n- 可与其他索引结合使用\n\n",
      "Source": "test2.md",
      "ChunkID": 8,
      "Metadata": {
        "nodeID": "8",
        "source": "test2.md"
      }
    },
    {
      "Content": "低内存占用\n- 可与其他索引结合使用\n\n\n\n## 第四章：RAG系统架构\n\n### 4.1 RAG概述\n\nRAG (Retrieval-Augmented Generation) 是一种结合检索和生成的AI架构，解决了LLM的关键问题：\n- 知识截止日期限制\n- 幻觉(Hallucination)问题\n- 无法访问私有数据\n\n### 4.2 RAG工作流程\n\n典型的RAG系统包含以下步骤：\n\n1. **文档处理**\n   - 加载原始文档\n   - 分块(Chunking)：500-1000字符/块\n   - 重叠(Overlap)：50-100字符保证连续性\n\n",
      "Source": "test2.md",
      "ChunkID": 9,
      "Metadata": {
        "nodeID": "9",
        "source": "test2.md"
      }
    },
    {
      "Content": " - 重叠(Overlap)：50-100字符保证连续性\n\n\n\n2. **向量化**\n   - 使用Embedding模型转换文本\n   - 存储到向量数据库\n   - 建立文档ID映射\n\n3. **查询阶段**\n   - 用户问题向量化\n   - 向量检索Top-K相关片段\n   - 重排序(可选)\n\n4. **生成阶段**\n   - 将检索内容注入Prompt\n   - LLM基于上下文生成回答\n   - 添加引用来源\n\n### 4.3 分块策略\n\n不同的分块策略适用于不同场景：\n\n",
      "Source": "test2.md",
      "ChunkID": 10,
      "Metadata": {
        "nodeID": "10",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd\n\n不同的分块策略适用于不同场景：\n\n\n\n**固定长度分块**\n- 最简单的方法\n- 可能切断语义完整性\n- 适合结构化文档\n\n**段落分块**\n- 按自然段落切分\n- 保持语义完整\n- 适合散文、文章\n\n**语义分块**\n- 使用NLP技术识别主题边界\n- 质量最高但计算成本大\n- 适合高质量要求场景\n\n**滑动窗口分块**\n- 固定大小+重叠\n- 平衡语义和效率\n- 最常用的方法\n\n## 第五章：实际应用场景\n\n### 5.1 智能客服\n\n",
      "Source": "test2.md",
      "ChunkID": 11,
      "Metadata": {
        "nodeID": "11",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd\ufffd章：实际应用场景\n\n### 5.1 智能客服\n\n\n\n向量检索可用于：\n- 快速匹配用户问题与FAQ库\n- 自动推荐相关帮助文档\n- 多轮对话上下文理解\n\n典型指标：\n- 意图识别准确率: 90%+\n- 响应时间: \u003c100ms\n- 用户满意度提升: 30%+\n\n### 5.2 代码搜索\n\n向量化代码片段，实现：\n- 语义代码搜索：用自然语言描述查找代码\n- 相似代码检测：发现重复逻辑\n- API推荐：根据使用场景推荐合适API\n\n### 5.3 推荐系统\n\n",
      "Source": "test2.md",
      "ChunkID": 12,
      "Metadata": {
        "nodeID": "12",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd\ufffd用场景推荐合适API\n\n### 5.3 推荐系统\n\n\n\n通过用户行为向量和内容向量：\n- 个性化内容推荐\n- 协同过滤增强\n- 冷启动问题缓解\n\n### 5.4 知识管理\n\n企业知识库应用：\n- 文档智能检索\n- 跨文档关联发现\n- 知识图谱构建辅助\n\n## 第六章：性能优化技巧\n\n### 6.1 索引优化\n\n- 选择合适的相似度度量\n- 调整HNSW的M和ef参数\n- 考虑混合索引策略\n\n### 6.2 查询优化\n\n",
      "Source": "test2.md",
      "ChunkID": 13,
      "Metadata": {
        "nodeID": "13",
        "source": "test2.md"
      }
    },
    {
      "Content": "- 考虑混合索引策略\n\n### 6.2 查询优化\n\n\n\n- 批量查询减少开销\n- 结果缓存机制\n- 预过滤+向量检索结合\n\n### 6.3 存储优化\n\n- 向量压缩(PQ/OPQ)\n- 分层存储：热数据SSD，冷数据HDD\n- 定期索引重建优化结构\n\n## 第七章：最佳实践\n\n### 7.1 Embedding模型选择\n\n考虑因素：\n- 维度：768维平衡性能和精度\n- 领域适配：通用vs垂直领域\n- 多语言支持\n- 推理速度\n\n",
      "Source": "test2.md",
      "ChunkID": 14,
      "Metadata": {
        "nodeID": "14",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffdvs垂直领域\n- 多语言支持\n- 推理速度\n\n\n\n推荐模型：\n- 英文：all-MiniLM-L6-v2, bge-large-en\n- 中文：bge-large-zh, m3e-base\n- 多语言：multilingual-e5-large\n\n### 7.2 系统架构建议\n\n- 读写分离：写入异步，读取实时\n- 增量更新机制\n- 监控和告警\n- 定期性能评估\n\n### 7.3 数据质量\n\n- 清洗文本：去除HTML、特殊字符\n- 保留关键元数据\n- 版本管理\n- 定期数据审计\n\n## 第八章：未来趋势\n\n### 8.1 混合检索\n\n",
      "Source": "test2.md",
      "ChunkID": 15,
      "Metadata": {
        "nodeID": "15",
        "source": "test2.md"
      }
    },
    {
      "Content": "# 第八章：未来趋势\n\n### 8.1 混合检索\n\n\n\n结合多种检索方式：\n- 稠密向量检索(Dense)\n- 稀疏向量检索(Sparse)\n- 关键词检索(BM25)\n- 加权融合\n\n### 8.2 多模态向量检索\n\n统一表示不同模态：\n- CLIP模型：图文联合检索\n- ImageBind：7种模态统一空间\n- 跨模态检索应用\n\n### 8.3 向量数据库即服务\n\n- 云原生架构\n- Serverless部署\n- 边缘计算支持\n- 自动扩缩容\n\n## 附录：常见问题\n\n",
      "Source": "test2.md",
      "ChunkID": 16,
      "Metadata": {
        "nodeID": "16",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd持\n- 自动扩缩容\n\n## 附录：常见问题\n\n\n\n**Q: 向量维度越高越好吗？**\nA: 不一定。高维度可能携带更多信息，但也带来：计算开销增加、过拟合风险、维度灾难。实践中768-1536维是较好的平衡点。\n\n**Q: 如何评估检索质量？**\nA: 常用指标包括：\n- Recall@K：前K个结果中相关文档比例\n- Precision@K：前K个结果中准确文档比例  \n- MRR：第一个相关结果的倒数排名\n- NDCG：考虑排序位置的累积增益\n\n",
      "Source": "test2.md",
      "ChunkID": 17,
      "Metadata": {
        "nodeID": "17",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd\ufffd名\n- NDCG：考虑排序位置的累积增益\n\n\n\n**Q: 向量数据库能替代传统数据库吗？**\nA: 不能完全替代。向量数据库擅长相似度搜索和语义理解，但传统数据库在事务处理、精确查询、关系建模方面仍有优势。两者应互补使用。\n\n**Q: 如何处理数据更新？**\nA: 常见策略：\n- 增量更新：新增/删除向量\n- 定期重建：适合批量更新\n- 版本控制：保留历史快照\n- 软删除：标记删除，延迟清理\n\n",
      "Source": "test2.md",
      "ChunkID": 18,
      "Metadata": {
        "nodeID": "18",
        "source": "test2.md"
      }
    },
    {
      "Content": "快照\n- 软删除：标记删除，延迟清理\n\n\n\n## 结语\n\n向量检索技术正在重新定义信息检索的范式。从传统的精确匹配到语义理解，从关键词搜索到多模态检索，向量数据库为AI应用提供了强大的基础设施。\n\n随着大语言模型的普及，RAG架构已成为企业级AI应用的标准范式。掌握向量检索技术，将帮助开发者构建更智能、更准确的AI系统。\n\n儿子今年 6 岁。\n爸爸比儿子大 30 岁。\n\n",
      "Source": "test2.md",
      "ChunkID": 19,
      "Metadata": {
        "nodeID": "19",
        "source": "test2.md"
      }
    },
    {
      "Content": "\ufffd子今年 6 岁。\n爸爸比儿子大 30 岁。\n\n\n\n---\n*文档版本: v1.0*  \n*最后更新: 2026年1月*  \n*关键词: 向量数据库, HNSW, RAG, 语义搜索, Embedding*\n\n",
      "Source": "test2.md",
      "ChunkID": 20,
      "Metadata": {
        "nodeID": "20",
        "source": "test2.md"
      }
    }
  ],
  "node_id_to_chunk_index": {
    "0": 0,
    "1": 1,
    "10": 10,
    "11": 11,
    "12": 12,
    "13": 13,
    "14": 14,
    "15": 15,
    "16": 16,
    "17": 17,
    "18": 18,
    "19": 19,
    "2": 2,
    "20": 20,
    "3": 3,
    "4": 4,
    "5": 5,
    "6": 6,
    "7": 7,
    "8": 8,
    "9": 9
  },
  "document_contents": {
    "/Users/wangyang/hello_langchain/langgraphdemogo/test2.md": "# 向量数据库技术白皮书\n\n## 第一章：向量数据库基础\n\n### 1.1 什么是向量数据库\n\n向量数据库是一种专门用于存储和检索高维向量数据的数据库系统。与传统关系型数据库不同，向量数据库通过计算向量之间的相似度来查找相关数据，而不是精确匹配。\n\n向量数据库的核心优势包括：\n- 语义搜索能力：能够理解查询的真实意图\n- 高维数据处理：可处理数百甚至上千维度的向量\n- 近似最近邻搜索：通过ANN算法实现毫秒级查询\n- 多模态支持：可以处理文本、图像、音频等多种数据类型\n\n### 1.2 向量表示\n\n在机器学习中，任何数据都可以被转换为向量表示。文本通过Embedding模型转换为稠密向量，这个过程称为向量化。常见的文本向量维度有：\n- Word2Vec: 100-300维\n- BERT系列: 768维或1024维\n- OpenAI ada-002: 1536维\n- nomic-embed-text: 768维\n\n向量化后，语义相近的内容在向量空间中距离更近。\n\n## 第二章：相似度度量方法\n\n### 2.1 欧氏距离 (L2 Distance)\n\n欧氏距离是最直观的距离度量方法，计算两点间的直线距离。公式为：\n\n$$d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n\n欧氏距离适用于关注绝对差异的场景，但对向量长度敏感。\n\n### 2.2 余弦相似度 (Cosine Similarity)\n\n余弦相似度衡量两个向量方向的相似程度，取值范围为[-1, 1]。公式为：\n\n$$\\text{cosine}(x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$$\n\n余弦相似度是文本检索最常用的度量方法，因为它：\n- 不受向量长度影响\n- 关注方向而非大小\n- 计算效率高\n\n余弦距离定义为：$d = 1 - \\text{cosine}(x, y)$\n\n### 2.3 点积 (Dot Product)\n\n点积相似度计算简单，适用于归一化向量。对于单位向量，点积等价于余弦相似度。\n\n$$\\text{dot}(x, y) = \\sum_{i=1}^{n} x_i \\cdot y_i$$\n\n## 第三章：向量索引算法\n\n### 3.1 HNSW算法原理\n\nHNSW (Hierarchical Navigable Small World) 是目前最先进的ANN算法之一，由Yury Malkov在2016年提出。\n\n#### 3.1.1 算法核心思想\n\nHNSW构建了一个多层图结构：\n- 底层(Layer 0)包含所有节点，形成稠密连接\n- 高层节点稀疏，作为\"高速公路\"加速搜索\n- 层数遵循指数分布：P(level) = (1/M)^level\n\n#### 3.1.2 关键参数\n\n**M参数**：每个节点的最大连接数\n- M越大，召回率越高，但内存和构建时间增加\n- 推荐值：4-64，常用16\n- 底层(Layer 0)最大连接数为2*M\n\n**efConstruction参数**：构建时的搜索宽度\n- 控制构建质量，值越大质量越好\n- 推荐值：100-500\n- 影响构建时间，但不影响查询速度\n\n**efSearch参数**：查询时的搜索宽度\n- 控制召回率与速度的平衡\n- 动态调整，可在查询时指定\n- 通常设为 max(k, efConstruction)\n\n#### 3.1.3 性能特点\n\nHNSW的时间复杂度：\n- 构建时间：O(N * log(N) * M * efConstruction)\n- 查询时间：O(log(N) * efSearch)\n- 空间复杂度：O(N * M)\n\n在100万向量、768维数据集上的典型性能：\n- QPS: 1000-5000\n- 召回率@10: 95%+\n- 平均查询延迟: 1-5ms\n\n### 3.2 其他索引算法对比\n\n**IVF (Inverted File Index)**\n- 基于聚类的分区方法\n- 查询时只搜索部分分区\n- 适合超大规模数据集\n- 召回率略低于HNSW\n\n**LSH (Locality Sensitive Hashing)**\n- 基于哈希的近似方法\n- 构建速度快，内存占用小\n- 召回率较低，适合对精度要求不高的场景\n\n**Product Quantization (PQ)**\n- 向量压缩技术\n- 大幅降低内存占用\n- 可与其他索引结合使用\n\n## 第四章：RAG系统架构\n\n### 4.1 RAG概述\n\nRAG (Retrieval-Augmented Generation) 是一种结合检索和生成的AI架构，解决了LLM的关键问题：\n- 知识截止日期限制\n- 幻觉(Hallucination)问题\n- 无法访问私有数据\n\n### 4.2 RAG工作流程\n\n典型的RAG系统包含以下步骤：\n\n1. **文档处理**\n   - 加载原始文档\n   - 分块(Chunking)：500-1000字符/块\n   - 重叠(Overlap)：50-100字符保证连续性\n\n2. **向量化**\n   - 使用Embedding模型转换文本\n   - 存储到向量数据库\n   - 建立文档ID映射\n\n3. **查询阶段**\n   - 用户问题向量化\n   - 向量检索Top-K相关片段\n   - 重排序(可选)\n\n4. **生成阶段**\n   - 将检索内容注入Prompt\n   - LLM基于上下文生成回答\n   - 添加引用来源\n\n### 4.3 分块策略\n\n不同的分块策略适用于不同场景：\n\n**固定长度分块**\n- 最简单的方法\n- 可能切断语义完整性\n- 适合结构化文档\n\n**段落分块**\n- 按自然段落切分\n- 保持语义完整\n- 适合散文、文章\n\n**语义分块**\n- 使用NLP技术识别主题边界\n- 质量最高但计算成本大\n- 适合高质量要求场景\n\n**滑动窗口分块**\n- 固定大小+重叠\n- 平衡语义和效率\n- 最常用的方法\n\n## 第五章：实际应用场景\n\n### 5.1 智能客服\n\n向量检索可用于：\n- 快速匹配用户问题与FAQ库\n- 自动推荐相关帮助文档\n- 多轮对话上下文理解\n\n典型指标：\n- 意图识别准确率: 90%+\n- 响应时间: \u003c100ms\n- 用户满意度提升: 30%+\n\n### 5.2 代码搜索\n\n向量化代码片段，实现：\n- 语义代码搜索：用自然语言描述查找代码\n- 相似代码检测：发现重复逻辑\n- API推荐：根据使用场景推荐合适API\n\n### 5.3 推荐系统\n\n通过用户行为向量和内容向量：\n- 个性化内容推荐\n- 协同过滤增强\n- 冷启动问题缓解\n\n### 5.4 知识管理\n\n企业知识库应用：\n- 文档智能检索\n- 跨文档关联发现\n- 知识图谱构建辅助\n\n## 第六章：性能优化技巧\n\n### 6.1 索引优化\n\n- 选择合适的相似度度量\n- 调整HNSW的M和ef参数\n- 考虑混合索引策略\n\n### 6.2 查询优化\n\n- 批量查询减少开销\n- 结果缓存机制\n- 预过滤+向量检索结合\n\n### 6.3 存储优化\n\n- 向量压缩(PQ/OPQ)\n- 分层存储：热数据SSD，冷数据HDD\n- 定期索引重建优化结构\n\n## 第七章：最佳实践\n\n### 7.1 Embedding模型选择\n\n考虑因素：\n- 维度：768维平衡性能和精度\n- 领域适配：通用vs垂直领域\n- 多语言支持\n- 推理速度\n\n推荐模型：\n- 英文：all-MiniLM-L6-v2, bge-large-en\n- 中文：bge-large-zh, m3e-base\n- 多语言：multilingual-e5-large\n\n### 7.2 系统架构建议\n\n- 读写分离：写入异步，读取实时\n- 增量更新机制\n- 监控和告警\n- 定期性能评估\n\n### 7.3 数据质量\n\n- 清洗文本：去除HTML、特殊字符\n- 保留关键元数据\n- 版本管理\n- 定期数据审计\n\n## 第八章：未来趋势\n\n### 8.1 混合检索\n\n结合多种检索方式：\n- 稠密向量检索(Dense)\n- 稀疏向量检索(Sparse)\n- 关键词检索(BM25)\n- 加权融合\n\n### 8.2 多模态向量检索\n\n统一表示不同模态：\n- CLIP模型：图文联合检索\n- ImageBind：7种模态统一空间\n- 跨模态检索应用\n\n### 8.3 向量数据库即服务\n\n- 云原生架构\n- Serverless部署\n- 边缘计算支持\n- 自动扩缩容\n\n## 附录：常见问题\n\n**Q: 向量维度越高越好吗？**\nA: 不一定。高维度可能携带更多信息，但也带来：计算开销增加、过拟合风险、维度灾难。实践中768-1536维是较好的平衡点。\n\n**Q: 如何评估检索质量？**\nA: 常用指标包括：\n- Recall@K：前K个结果中相关文档比例\n- Precision@K：前K个结果中准确文档比例  \n- MRR：第一个相关结果的倒数排名\n- NDCG：考虑排序位置的累积增益\n\n**Q: 向量数据库能替代传统数据库吗？**\nA: 不能完全替代。向量数据库擅长相似度搜索和语义理解，但传统数据库在事务处理、精确查询、关系建模方面仍有优势。两者应互补使用。\n\n**Q: 如何处理数据更新？**\nA: 常见策略：\n- 增量更新：新增/删除向量\n- 定期重建：适合批量更新\n- 版本控制：保留历史快照\n- 软删除：标记删除，延迟清理\n\n## 结语\n\n向量检索技术正在重新定义信息检索的范式。从传统的精确匹配到语义理解，从关键词搜索到多模态检索，向量数据库为AI应用提供了强大的基础设施。\n\n随着大语言模型的普及，RAG架构已成为企业级AI应用的标准范式。掌握向量检索技术，将帮助开发者构建更智能、更准确的AI系统。\n\n儿子今年 6 岁。\n爸爸比儿子大 30 岁。\n\n---\n*文档版本: v1.0*  \n*最后更新: 2026年1月*  \n*关键词: 向量数据库, HNSW, RAG, 语义搜索, Embedding*"
  },
  "dimension": 768,
  "created_at": "2026-01-30T11:30:39+08:00",
  "updated_at": "2026-01-30T11:30:39+08:00"
}